---
title: "Data transformation using dplyr (aka five verbs)"
author: "Taavi Päll"
date: "02. April 2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
I our previous classes we have been working with small cleaned up dataset to go through steps of creating some of the most common visualization types. 

In your workflow you are going to need data visualization at two points, namely during exploratory data analysis where you learn to know your dataset and during report preparation when you try to communicate what have you found. And this is not two stop trip, it's more like a roundabout, an iterative process, where you pass these two point multiple times after you have done some "tweaking" of your data. By "tweaking" I mean here data transformation and/or modeling. 

You need to transform your data during analysis, because in real life you rarely start with a dataset that is in the right form for visualization and modeling. So, often you will need to:

- summarise your data or to 
- create new variables, 
- rename variables, or 
- reorder the observations. 

We are going to use the dplyr library from tidyverse to learn how to carry out these tasks. 

## Sources
Again, we are going to follow closely R4DS book chapter "Data transformation" available from http://r4ds.had.co.nz/transform.html. More examples are available from https://rstats-tartu.github.io/lectures/tidyverse.html#dplyr-ja-selle-viis-verbi

## Class III

Load libraries and datasets:
```{r}
library(tidyverse)
```

However, instead of nycflights13 data we are going to use Estonian apartment transactions data. Transactions data contain monthly apartment sales data from January 2005 to January 2017 split up by counties and size of apartments. Price info is available when at least five transactions has been carried out.
```{r}
(transactions <- read_csv(file = "data/transactions.csv"))
```

## dplyr basics

Most of the data transformation tasks can be carried out using five verbs from dplyr library:

- Pick observations by their values (filter()).
- Reorder the rows (arrange()).
- Pick variables by their names (select()).
- Create new variables with functions of existing variables (mutate()).
- Collapse many values down to a single summary (summarise()).

- These can all be used in conjunction with group_by() which changes the scope of each function from operating on the entire dataset to operating on it group-by-group. 


These six functions provide the verbs for a language of data manipulation.

All verbs work similarly:

The first argument is a data frame.

The subsequent arguments describe what to do with the data frame, using the variable names (without quotes).

The result is a new data frame.

Together these properties make it easy to chain together multiple simple steps to achieve a complex result. Let’s dive in and see how these verbs work.

## Filter rows with filter()

filter() allows you to subset observations based on their values. The first argument is the name of the data frame. The second and subsequent arguments are the expressions that filter the data frame. 

For example, we can select data on January 2005 with:
```{r}
filter(transactions, year == 2005, month == 1)
```

dplyr runs the filtering operation and returns a new data frame. dplyr functions never modify their inputs, so if you want to save the result, you'll need to use the assignment operator, <-:

```{r}
jan2005 <- filter(transactions, year == 2005, month == 1)
```

### Comparisons

What is this == operator above? Why not use = to check equality:
```{r, eval=FALSE}
filter(transactions, year = 2005)
```

It appears that = is another assignment operator besides ->

There's another common problem you might encounter when using ==: floating point numbers. Although, theoretically TRUE, following comparisons return FALSE!
```{r}
sqrt(2) ^ 2 == 2
1/49 * 49 == 1
```

This is because computers and R use finite precision arithmetic and cannot store an infinite number of digits.

This can be overcome by using near() function instead of ==:
```{r}
near(sqrt(2) ^ 2,  2)
near(1 / 49 * 49, 1)
```

### Logical operators

Multiple comparisons within filter() function are combined with comma "," which means "and" (&). In case of "and" all comparisons must evaluate to TRUE for observations to be returned.

Together, logical (boolean) operators are:

- & is AND, 
- | is OR, 
- ! is NOT

The following code finds all transactions in November OR December:
```{r}
filter(transactions, month == 11 | month == 12)
```

You can’t write filter(flights, month == 11 | 12) and in case of numeric months this will give you wrong answer instead of Error, so be careful:
```{r, eval=FALSE}
filter(transactions, month == 11 | 12)
```

A useful short-hand for this problem is x %in% y. This will select every row where x is one of the values in y:

```{r}
(nov_dec <- filter(transactions, month %in% c(11, 12)))
```

Sometimes you can simplify complicated subsetting by remembering De Morgan's law: !(x & y) is the same as !x | !y, and !(x | y) is the same as !x & !y. For example, if you wanted to find flights that weren’t delayed (on arrival or departure) by more than two hours, you could use either of the following two filters:
```{r}
filter(transactions, !(price_min > 1000 | price_max > 1000))
filter(transactions,  price_min <= 1000, price_max <= 1000)
```

### Missing values

One important feature of R that can make comparison tricky are missing values, or NAs ("not availables"). NA represents an unknown value so missing values are "contagious": almost any operation involving an unknown value will also be unknown.
```{r}
NA > 5
10 == NA
NA + 10
NA / 2
```

As Rsudio already might suggest, if you want to determine if a value is missing, use is.na():
```{r}
x <- NA
is.na(x)
```

Let's use is.na() within filter to remove rows with missing price info:
```{r}
filter(transactions, is.na(price_total))
```

Ok. Now we got all rows with missing price_total... how would you change this code to really exclude these rows with missing data:
```{r}
!FALSE
filter(transactions, !is.na(price_total))
```

There is another function that works with data frames to find rows with complete set of observations - complete.cases():
```{r}
transactions %>% filter(complete.cases(.))
```

### Exercises - homework

1. Find all transactions that

- Had an area_mean more than one hundred square meters
- Took place in Saare maakond
- Were done during summer (July, August, and September)
- Another useful dplyr filtering helper is between(). What does it do?

2. How many rows have a missing total_price? What other variables are missing?

3. Why is NA ^ 0 not missing? Why is NA | TRUE not missing? Why is FALSE & NA not missing? Can you figure out the general rule? (NA * 0 is a tricky counterexample!)
```{r}
NA ^ 0
NA | TRUE
```

## Arrange rows with arrange()
arrange() works similarly to filter() except that instead of selecting rows, it changes their order.
It takes a data frame and a set of column names to order by. If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns:
```{r}
arrange(transactions, price_unit_area_max)
```

Use desc() to re-order by a column in descending order:
```{r}
arrange(transactions, desc(price_unit_area_max))
```

Missing values are always sorted at the end, even with desc() function:
```{r}
df <- tibble(x = c(5, 2, NA))
arrange(df, x)
```

```{r}
arrange(df, desc(x))
```

### Exercises

1. How could you use arrange() to sort all missing values to the start? (Hint: use is.na()).

2. Sort transactions to find the busiest months in each county.

3. Sort transactions to find the months with highest mean price payed for price_unit_area_mean.

4. Which apartements (size) sold the best in 2017? Which sold worst in 2017?

## Select columns with select()
select() allows you to rapidly zoom in on a useful subset of columns using operations based on the names of the variables.

Select three columns:
```{r}
select(transactions, year, month, transactions)
```

Select columns from year to transactions:
```{r}
select(transactions, year:transactions)
```

Exlude columns from area_total to title:
```{r}
select(transactions, -(area_total:title))
```

There are a number of __helper functions you can use within select()__:

- starts_with("abc"): matches names that begin with "abc".

- ends_with("xyz"): matches names that end with "xyz".

- contains("ijk"): matches names that contain "ijk".

- matches("(.)\\1"): selects variables that match a regular expression. This one matches any variables that contain repeated characters. You’ll learn more about regular expressions in strings.

- num_range("V", 1:3) matches V1, V2 and V3.

- everything() is useful if you have a handful of variables you'd like to move to the start of the data frame.

See ?select for more details.

Move column "title" to the start of the data frame.
```{r}
select(transactions, title, everything())
```

### Exercises

1. What happens if you include the name of a variable multiple times in a select() call?

2. What does the one_of() function do? Why might it be helpful in conjunction with this vector?
```{r}
vars <- c("year", "month", "county", "area", "price_unit_area_mean", "consumer_index")
```

3. Select from 'transactions' all variables that contain string 'PRICE' (note case!). 

Does the result of running the following code surprise you? How do the select helpers deal with case by default? How can you change that default?
```{r}

```

## Add new variables with mutate()
Mutate creates new variables (columns) from existing variables (columns). Mutate is used to do calculations columnwise.

To start illustrating what mutate does, let's have a look at the changes of real estate prices in transactions data. 

In Estonia, real estate market collapsed after 2008 economic crisis. In 2017, various sources started talking about the new bubble. Are we really near the bubble? We can start elucidating this by looking at the price dynamics.

First, select smaller subset of columns: year to transactions, median price per unit area (ends with median) and consumer index.
```{r}
med <- select(transactions, year:transactions, ends_with("median"), consumer_index)
med
```

Let's make a quick plot using Harjumaa data. 

Oh, but we want to have time on the x axis. Whereas we have year and month columns in our dataset and these are not in the format that is recognised as time...

We need to convert year and month to time format in the form "2005 Jan" or something..

We are going to use ymd() function from tidyverse lubridate library. ymd recognizes several types of year-month-day strings (like "2018-04-09", "2018 04 09", etc) and converts them to date class. We just have to add also place holder for day for this function to work. We would set day to 1, as first day of month.

First, we construct date string with paste() and then convert this string to date:
```{r}
library(lubridate)
med <- med %>% 
  mutate(date_string = paste(year, month, 1),
         date = ymd(date_string)) %>% 
  select(date, date_string, everything())
med
```

Let's plot price trend using median price per $m^2$ (price_unit_area_median) placing each county on separate facet: 
```{r}
med %>% 
  ggplot() +
  geom_line(mapping = aes(x = date, y = price_unit_area_median, colour = area)) +
  facet_wrap(~ county)
```

You can check if x = time_string works in ggplot.

We can see that in "Harju maakond" and "Tartu maakond" the nominal prices are reaching again the levels before the economic crisis. The question is, whether there is still some room to growth?

We can try to answer this question by bringing in also the inflation that helps to understand the real prices. We can adjust prices for inflation by normalizing them with consumer index. Luckily our transactions dataset already contains consumer indices as percent relative to year 2005 (code creating this dataset is available in file "data-raw/download-apartment-data.R").

Consumer index is expressed as percent relative to year 2005:
```{r}
mutate(transactions_price,
  consumer_index = consumer_index / 100,
  adj_price = price_unit_area_median * consumer_index
)
```




