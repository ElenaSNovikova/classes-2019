---
title: "Data transformation using dplyr (aka five verbs)"
author: "Taavi Päll"
date: "02. April 2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
I our previous classes we have been working with small cleaned up dataset to go through steps of creating some of the most common visualization types. 

In your workflow you are going to need data visualization at two points, namely during exploratory data analysis where you learn to know your dataset and during report preparation when you try to communicate what have you found. And this is not two stop trip, it's more like a roundabout, an iterative process, where you pass these two point multiple times after you have done some "tweaking" of your data. By "tweaking" I mean here data transformation and/or modeling. 

You need to transform your data during analysis, because in real life you rarely start with a dataset that is in the right form for visualization and modeling. So, often you will need to:

- summarise your data or to 
- create new variables, 
- rename variables, or 
- reorder the observations. 

We are going to use the dplyr library from tidyverse to learn how to carry out these tasks. 

## Sources
Again, we are going to follow closely R4DS book chapter "Data transformation" available from http://r4ds.had.co.nz/transform.html. More examples are available from https://rstats-tartu.github.io/lectures/tidyverse.html#dplyr-ja-selle-viis-verbi

## Class III

Load libraries and datasets:
```{r}
library(tidyverse)
```

However, instead of nycflights13 data we are going to use Estonian apartment transactions data. Transactions data contain monthly apartment sales data from January 2005 to January 2017 split up by counties and size of apartments. Price info is available when at least five transactions has been carried out.
```{r}
(transactions <- read_csv(file = "data/transactions.csv"))
```

## dplyr basics

Most of the data transformation tasks can be carried out using five verbs from dplyr library:

- Pick observations by their values (filter()).
- Reorder the rows (arrange()).
- Pick variables by their names (select()).
- Create new variables with functions of existing variables (mutate()).
- Collapse many values down to a single summary (summarise()).

- These can all be used in conjunction with group_by() which changes the scope of each function from operating on the entire dataset to operating on it group-by-group. 


These six functions provide the verbs for a language of data manipulation.

All verbs work similarly:

The first argument is a data frame.

The subsequent arguments describe what to do with the data frame, using the variable names (without quotes).

The result is a new data frame.

Together these properties make it easy to chain together multiple simple steps to achieve a complex result. Let’s dive in and see how these verbs work.

## Filter rows with filter()

filter() allows you to subset observations based on their values. The first argument is the name of the data frame. The second and subsequent arguments are the expressions that filter the data frame. 

For example, we can select data on January 2005 with:
```{r}
filter(transactions, year == 2005, month == 1)
```

dplyr runs the filtering operation and returns a new data frame. dplyr functions never modify their inputs, so if you want to save the result, you'll need to use the assignment operator, <-:

```{r}
jan2005 <- filter(transactions, year == 2005, month == 1)
```

### Comparisons

What is this == operator above? Why not use = to check equality:
```{r, eval=FALSE}
filter(transactions, year = 2005)
```

It appears that = is another assignment operator besides ->

There's another common problem you might encounter when using ==: floating point numbers. Although, theoretically TRUE, following comparisons return FALSE!
```{r}
sqrt(2) ^ 2 == 2
1/49 * 49 == 1
```

This is because computers and R use finite precision arithmetic and cannot store an infinite number of digits.

This can be overcome by using near() function instead of ==:
```{r}
near(sqrt(2) ^ 2,  2)
near(1 / 49 * 49, 1)
```

### Logical operators

Multiple comparisons within filter() function are combined with comma "," which means "and" (&). In case of "and" all comparisons must evaluate to TRUE for observations to be returned.

Together, logical (boolean) operators are:

- & is AND, 
- | is OR, 
- ! is NOT

The following code finds all transactions in November OR December:
```{r}
filter(transactions, month == 11 | month == 12)
```

You can’t write filter(flights, month == 11 | 12) and in case of numeric months this will give you wrong answer instead of Error, so be careful:
```{r, eval=FALSE}
filter(transactions, month == 11 | 12)
```

A useful short-hand for this problem is x %in% y. This will select every row where x is one of the values in y:

```{r}
(nov_dec <- filter(transactions, month %in% c(11, 12)))
```

Sometimes you can simplify complicated subsetting by remembering De Morgan's law: !(x & y) is the same as !x | !y, and !(x | y) is the same as !x & !y. For example, if you wanted to find flights that weren’t delayed (on arrival or departure) by more than two hours, you could use either of the following two filters:
```{r}
filter(transactions, !(price_min > 1000 | price_max > 1000))
filter(transactions,  price_min <= 1000, price_max <= 1000)
```

### Missing values

One important feature of R that can make comparison tricky are missing values, or NAs ("not availables"). NA represents an unknown value so missing values are "contagious": almost any operation involving an unknown value will also be unknown.
```{r}
NA > 5
10 == NA
NA + 10
NA / 2
```

As Rsudio already might suggest, if you want to determine if a value is missing, use is.na():
```{r}
x <- NA
is.na(x)
```

Let's use is.na() within filter to remove rows with missing price info:
```{r}
filter(transactions, is.na(price_total))
```

Ok. Now we got all rows with missing price_total... how would you change this code to really exclude these rows with missing data:
```{r}
!FALSE
filter(transactions, !is.na(price_total))
```

There is another function that works with data frames to find rows with complete set of observations - complete.cases():
```{r}
transactions %>% filter(complete.cases(.))
```

### Exercises - homework

1. Find all transactions that

- Had an area_mean more than one hundred square meters
- Took place in Saare maakond
- Were done during summer (July, August, and September)
- Another useful dplyr filtering helper is between(). What does it do?

```{r}
filter(transactions, area_mean > 100, county == "Saare maakond", between(month, 7, 9))
```


2. How many rows have a missing total_price? What other variables are missing?

```{r}
filter(transactions, is.na(price_total))
```


3. Why is NA ^ 0 not missing? Why is NA | TRUE not missing? Why is FALSE & NA not missing? Can you figure out the general rule? (NA * 0 is a tricky counterexample!)
```{r}
NA ^ 0
NA | TRUE
```

```{r}
NA * 0
NA * FALSE
```


## Arrange rows with arrange()
arrange() works similarly to filter() except that instead of selecting rows, it changes their order.
It takes a data frame and a set of column names to order by. If you provide more than one column name, each additional column will be used to break ties in the values of preceding columns:
```{r}
arrange(transactions, price_unit_area_mean)
```

Use desc() to re-order by a column in descending order:
```{r}
arrange(transactions, desc(price_unit_area_mean))
```

Missing values are always sorted at the end, even with desc() function:
```{r}
df <- tibble(x = c(5, 2, NA))
arrange(df, x)
```

```{r}
arrange(df, desc(x))
```

### Exercises

1. How could you use arrange() to sort all missing values to the start? (Hint: use is.na()).

```{r}
arrange(df, desc(is.na(x)))
arrange(df, !is.na(x))
```


2. Sort transactions to find the busiest months in each county.

```{r}
arrange(transactions, desc(county), desc(transactions))
```

We'll come to this question back later...

3. Sort transactions to find the months with highest mean price payed for price_unit_area_mean.

```{r}
arrange(transactions, desc(month), desc(price_unit_area_mean))
```

come back again...


4. Which apartements (size) sold the best in 2017? Which sold worst in 2017?

```{r}
arrange(filter(transactions, year == 2017), desc(transactions))
```


## Select columns with select()
select() allows you to rapidly zoom in on a useful subset of columns using operations based on the names of the variables.

Select three columns:
```{r}
select(transactions, year, month, transactions)
```

Select columns from year to transactions:
```{r}
select(transactions, year:transactions)
```

```{r}
select(transactions, 1:5)
```


Exlude columns from area_total to title:
```{r}
select(transactions, -(area_total:title))
```

There are a number of __helper functions you can use within select()__:

- starts_with("abc"): matches names that begin with "abc".

- ends_with("xyz"): matches names that end with "xyz".

- contains("ijk"): matches names that contain "ijk".

- matches("(.)\\1"): selects variables that match a regular expression. This one matches any variables that contain repeated characters. You’ll learn more about regular expressions in strings.

```{r, eval = FALSE}
matches("^abc") # same as starts_with("abc)
matches("xyz$") # same as ends_with("abc)
matches("ijk") # same as contains("ijk)
```


- num_range("V", 1:3) matches V1, V2 and V3.

- everything() is useful if you have a handful of variables you'd like to move to the start of the data frame.

See ?select for more details.

Move column "title" to the start of the data frame.
```{r}
select(transactions, title, everything())
```

### Exercises

1. What happens if you include the name of a variable multiple times in a select() call?

```{r}
select(transactions, area_total, area_total)
```


2. What does the one_of() function do? Why might it be helpful in conjunction with this vector?
```{r}
(vars <- c("year", "month", "county", "area", "price_unit_area_mean", "consumer_index", "foo"))
select(transactions, one_of(vars))
```

3. Select from 'transactions' all variables that contain string 'PRICE' (note case!). 

```{r}
select(transactions, contains("PRICE"))
```


Does the result of running the following code surprise you? How do the select helpers deal with case by default? How can you change that default?
```{r}
?select
select(transactions, contains("PRICE", ignore.case = FALSE))
```

## Add new variables with mutate()
Mutate creates new variables (columns) from existing variables (columns). Mutate is used to do calculations columnwise.

To start illustrating what mutate does, let's have a look at the changes of real estate prices in transactions data. 

In Estonia, real estate market collapsed after 2008 economic crisis. In 2017, various sources started talking about the new bubble. Are we really near the bubble? We can start elucidating this by looking at the price dynamics.

First, select smaller subset of columns: year to transactions, median price per unit area (ends with median) and consumer index.
```{r}
med <- select(transactions, year:transactions, ends_with("median"), consumer_index)
med
```

Let's make a quick plot using Harjumaa data. 

Oh, but we want to have time on the x axis. Whereas we have year and month columns in our dataset and these are not in the format that is recognised as time...

We need to convert year and month to time format in the form "2005 Jan" or something..

We are going to use ymd() function from tidyverse lubridate library. ymd recognizes several types of year-month-day strings (like "2018-04-09", "2018 04 09", etc) and converts them to date class. We just have to add also place holder for day for this function to work. We would set day to 1, as first day of month.

First, we construct date string with paste() and then convert this string to date. Note that we can refer to columns that we've just created:

```{r}
paste("a", "b", "foo", sep = "-")
paste0("a", "b", "foo")
```


```{r}
library(lubridate)
med <- med %>% 
  mutate(date_string = paste(year, month, 1),
         date = ymd(date_string)) %>% 
  select(date_string, date, everything())
med
```

Let's plot price trend using median price per $m^2$ (price_unit_area_median) placing each county on separate facet: 
```{r}
ggplot(data = med) +
  geom_line(mapping = aes(x = date, y = price_unit_area_median, color = area)) +
  facet_wrap(~ county)
```

You can check if x = time_string works in ggplot.

We can see that in "Harju maakond" and "Tartu maakond" the nominal prices are reaching again the levels before the economic crisis. The question is, whether there is still some room to growth?

We can try to answer this question by bringing in also the inflation that helps to understand the real prices. We can adjust prices for inflation by normalizing them with consumer index. Luckily our transactions dataset already contains consumer indices as percent relative to year 2005 (code used to create this dataset is available in file "data-raw/download-apartment-data.R").

Consumer index is expressed as percent relative to year 2005, first we need to convert to proportions and then we can calculate adjusted prices, like so:
```{r}
(med_adj <- mutate(med,
  consumer_index = consumer_index / 100,
  adj_price = price_unit_area_median / consumer_index
) %>% 
  select(adj_price, price_unit_area_median, everything()))
```

Plot out adjusted prices:
```{r}
ggplot(data = med_adj) +
  geom_line(mapping = aes(x = date, y = adj_price, color = area)) +
  facet_wrap(~ county)
```

As we can see, based on real values, there was still some potential for growth in 2017. 

But what's the glich at the very end of the lines? We try to find that out using arrange..

```{r}
arrange(med_adj, desc(adj_price)) %>% 
  select(adj_price, price_unit_area_median, consumer_index, everything())
```


Let's fix previous plot by using filter:

```{r}
ggplot(data = filter(med_adj, consumer_index > 0)) +
  geom_line(mapping = aes(x = date, y = adj_price, color = area)) +
  facet_wrap(~ county)
```


You can keep only newly created variable by using transmute() function:
```{r}
transmute(transactions, 
          adj_price = price_unit_area_median / consumer_index)
```


## Useful creation functions

There are many functions for creating new variables that you can use with mutate(). 

> The key property is that the function must be vectorised: it must take a vector of values as input, return a vector with the same number of values as output. 

Some of the functions are familiar from base R class: 
- Arithmetic operators: +, -, *, /, ^.

Arithmetic operators are also useful in conjunction with the aggregate functions. For example, x / sum(x) calculates the proportion of a total, and y - mean(y) computes the difference from the mean.

- Modular arithmetic: %/% (integer division) and %% (remainder, modulus), where x == y * (x %/% y) + (x %% y). Modular arithmetic is a handy tool because it allows you to break integers up into pieces. For example, in the tidyverse flights dataset, you can compute hour and minute from dep_time with:

To demonstrate modulo and integer division (if you figure out meaningful use of these two operators in transactions, let me know), let's quickly introduce new dataset:
```{r}
library(nycflights13)
flights
```

Here we are, modulo and integer arithmetic operators to wrangle data, note the use of transmute:
```{r}
transmute(flights,
  dep_time,
  hour = dep_time %/% 100,
  minute = dep_time %% 100
)
```

- Logs: log(), log2(), log10(). Note that log() converts to natural logarithm.

To demonstrate logs in action, we can look at the prices. Seems that they are not normally distributed... and most of the values are concentrated at the left side.
```{r}
ggplot(data = transactions) +
  geom_histogram(mapping = aes(x = price_max))
```

We can have better look at the distibution by using log transformaton.
```{r}
price <- transmute(transactions, price_max_log = log10(price_max))
ggplot(data = price) +
  geom_histogram(mapping = aes(x = price_max_log))
```


```{r}
log(0 + 0.01)
```


- Offsets: lead() and lag() allow you to refer to leading or lagging values. This allows you to compute running differences (e.g. x - lag(x)) or find when values change (x != lag(x)).

```{r}
(x <- 1:10)
lag(x)
?lag
lead(x)
```

- Cumulative and rolling aggregates: R provides functions for running sums, products, mins and maxes: cumsum(), cumprod(), cummin(), cummax(); 
and dplyr provides cummean() for cumulative means.

```{r}
cumsum(x)
cummean(x)
```

- Ranking: there are a number of ranking functions, but you should start with min_rank(). It does the most usual type of ranking (e.g. 1st, 2nd, 2nd, 4th).

```{r}
(y <- c(1, 2, 2, NA, 3, 4))
min_rank(y)
min_rank(desc(y))
```

There are also row_number(), dense_rank(), percent_rank(), cume_dist(), ntile():
```{r}
row_number(y)
dense_rank(y)
percent_rank(y)
cume_dist(y)
```

### Exercises

- Currently dep_time and sched_dep_time are convenient to look at, but hard to compute with because they're not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight.

```{r}
flights
```

### This goes to homework
- Compare air_time with arr_time - dep_time. What do you expect to see? What do you see? What do you need to do to fix it?
```{r}
transmute(flights, air_time, flight_time = arr_time - dep_time)
```

```{r}
flights
```


- Compare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related?
```{r}
flights
```

- Find the 10 most delayed flights using a ranking function. How do you want to handle ties? Carefully read the documentation for min_rank().

- What does 1:3 + 1:10 return? Why?
```{r}

```


- What trigonometric functions does R provide?

## Grouped summaries with summarise()

The last key verb is summarise(). It collapses a data frame to a single row:
```{r}
summarise(transactions, 
          transactions = mean(transactions),
          price_total = mean(price_total, na.rm = TRUE)
          )
```

summarise() is not terribly useful unless we pair it with group_by(). This changes the unit of analysis from the complete dataset to individual groups. 

Then, when you use the dplyr verbs on a grouped data frame they'll be automatically applied "by group". 

For example, if we applied exactly the same code to a data frame grouped by county, we get the average price volume of transactions per county:
```{r}
(by_county <- group_by(transactions, county))
```

```{r}
summarise(by_county, 
          transactions = mean(transactions),
          price_total = mean(price_total, na.rm = TRUE)
          )
```


> Together group_by() and summarise() provide one of the tools that you'll use most commonly when working with dplyr: grouped summaries. 

## Combining multiple operations with the pipe

Imagine that we want to explore the mean_volume_per_month per county for large apartments (area == "70-249,99"). Using what you know about dplyr, you might write code like this:

```{r}
by_county <- group_by(transactions, county, area)
volume <- summarise(by_county, mean_volume_per_month = mean(price_total, na.rm = TRUE))
volume <- filter(volume, area == "70-249,99")
(volfirst <- select(volume, mean_volume_per_month, everything()))
```


```{r}
transactions %>% 
  group_by(county, area) %>% 
  summarise(mean_volume_per_month = mean(price_total, na.rm = TRUE)) %>% 
  filter(area == "70-249,99") %>% 
  select(mean_volume_per_month, everything()) %>% 
  ggplot() +
  geom_col(mapping = aes(x = county, y = mean_volume_per_month)) +
  coord_flip() +
  labs(x = "")
```



There are four steps to prepare this data:

- Group transactions by county and area.

- Summarise to compute total volume.

- Filter to include only large 70-245 m2 apartments.

- we rearranged column order by select + everything


This code is a little frustrating to write because we have to give each intermediate data frame a name, even though we don't care about it. Naming things is hard, so this slows down our analysis.

There's another way to tackle the same problem with the pipe, %>%:


```{r}
volume <- transactions %>% 
  group_by(county, area) %>% 
  summarise(mean_volume_per_month = mean(price_total, na.rm = TRUE)) %>% 
  filter(area == "70-249,99")
volume
```


This focuses on the transformations, not what's being transformed, which makes the code easier to read. You can read it as a series of imperative statements: group, then summarise, then filter. As suggested by this reading, a good way to pronounce %>% when reading code is "then".

> Behind the scenes, x %>% f(y) turns into f(x, y), and x %>% f(y) %>% g(z) turns into g(f(x, y), z) and so on. You can use the pipe to rewrite multiple operations in a way that you can read left-to-right, top-to-bottom. 

## Missing values
What about this na.rm argument in summary function. What happens if we don't set it?
```{r}

```

We get a lot of missing values! That's because aggregation functions obey the usual rule of missing values: if there’s any missing value in the input, the output will be a missing value. 

## Counts

Whenever you do any aggregation, it's always a good idea to include either a count (n()), or a count of non-missing values (sum(!is.na(x))).

```{r}
transactions %>% 
  group_by(county, area) %>% 
  summarise(Av_total_price = mean(price_total, na.rm = TRUE),
            N = n())
```


## Useful summary functions

Just using means, counts, and sum can get you a long way, but R provides many other useful summary functions:

- Measures of location: we've used mean(x), but median(x) is also useful. It's sometimes useful to combine aggregation with logical subsetting.



